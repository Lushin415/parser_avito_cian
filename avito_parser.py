import asyncio
import html
import json
import random
import re
import time
from urllib.parse import unquote, urlparse, parse_qs, urlencode, urlunparse
from datetime import datetime, timedelta

from bs4 import BeautifulSoup
from curl_cffi import requests
from loguru import logger
from pydantic import ValidationError
from requests.cookies import RequestsCookieJar

from common_data import HEADERS
from db_service import SQLiteDBHandler
from dto import Proxy, AvitoConfig
from get_cookies import get_cookies
from hide_private_data import log_config
from load_config import load_avito_config
from models import ItemsResponse, Item
from tg_sender import SendAdToTg
from vk_sender import SendAdToVK
from version import VERSION
from xlsx_service import XLSXHandler

DEBUG_MODE = False

logger.add("logs/app.log", rotation="5 MB", retention="5 days", level="DEBUG")


class AvitoParse:
    def __init__(
            self,
            config: AvitoConfig,
            stop_event=None
    ):
        self.config = config
        self.proxy_obj = self.get_proxy_obj()
        self.db_handler = SQLiteDBHandler()
        self.tg_handler = self.get_tg_handler()
        self.vk_handler = self.get_vk_handler()
        self.xlsx_handler = XLSXHandler(self.__get_file_title())
        self.stop_event = stop_event
        self.cookies = None
        self.session = requests.Session()
        self.headers = HEADERS
        self.good_request_count = 0
        self.bad_request_count = 0

        log_config(config=self.config, version=VERSION)

    def _parse_area_from_description(self, ads: list[Item]) -> list[Item]:
        """–ü–∞—Ä—Å–∏—Ç –ø–ª–æ—â–∞–¥—å –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        for ad in ads:
            if ad.description and not ad.total_meters:
                ad.total_meters = extract_area_from_description(ad.description)
        return ads

    def get_tg_handler(self) -> SendAdToTg | None:
        if all([self.config.tg_token, self.config.tg_chat_id]):
            return SendAdToTg(bot_token=self.config.tg_token, chat_id=self.config.tg_chat_id)
        return None

    def _send_to_tg(self, ads: list[Item]) -> None:
        for ad in ads:
            self.tg_handler.send_to_tg(ad=ad)

    def get_vk_handler(self) -> SendAdToVK | None:
        if all([self.config.vk_token, self.config.vk_user_id]):
            logger.info("VK handler –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return SendAdToVK(vk_token=self.config.vk_token, user_id=self.config.vk_user_id)
        logger.debug("VK handler –Ω–µ —Å–æ–∑–¥–∞–Ω: vk_token –∏–ª–∏ vk_user_id –Ω–µ –∑–∞–¥–∞–Ω—ã –≤ –∫–æ–Ω—Ñ–∏–≥–µ")
        return None

    def _send_to_vk(self, ads: list[Item]) -> None:
        logger.debug(f"VK: –æ—Ç–ø—Ä–∞–≤–ª—è—é {len(ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        for ad in ads:
            self.vk_handler.send_to_vk(ad=ad)
            time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –¥–ª—è VK

    def get_proxy_obj(self) -> Proxy | None:
        if all([self.config.proxy_string, self.config.proxy_change_url]):
            return Proxy(
                proxy_string=self.config.proxy_string,
                change_ip_link=self.config.proxy_change_url
            )
        logger.info("–†–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –ø—Ä–æ–∫—Å–∏")
        return None

    def get_cookies(self, max_retries: int = 1, delay: float = 2.0) -> dict | None:
        if not self.config.use_webdriver:
            return

        for attempt in range(1, max_retries + 1):
            if self.stop_event and self.stop_event.is_set():
                return None

            try:
                cookies, user_agent = asyncio.run(
                    get_cookies(proxy=self.proxy_obj, headless=True, stop_event=self.stop_event))
                if cookies:
                    logger.info(f"[get_cookies] –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã cookies —Å –ø–æ–ø—ã—Ç–∫–∏ {attempt}")

                    self.headers["user-agent"] = user_agent
                    return cookies
                else:
                    raise ValueError("–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç cookies")
            except Exception as e:
                logger.warning(f"[get_cookies] –ü–æ–ø—ã—Ç–∫–∞ {attempt} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt < max_retries:
                    time.sleep(delay * attempt)  # —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
                else:
                    logger.error(f"[get_cookies] –í—Å–µ {max_retries} –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å")
                    return None

    def save_cookies(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç cookies –∏–∑ requests.Session –≤ JSON-—Ñ–∞–π–ª."""
        with open("cookies.json", "w") as f:
            json.dump(self.session.cookies.get_dict(), f)

    def load_cookies(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç cookies –∏–∑ JSON-—Ñ–∞–π–ª–∞ –≤ requests.Session."""
        try:
            with open("cookies.json", "r") as f:
                cookies = json.load(f)
                jar = RequestsCookieJar()
                for k, v in cookies.items():
                    jar.set(k, v)
                self.session.cookies.update(jar)
        except FileNotFoundError:
            pass

    def fetch_data(self, url, retries=3, backoff_factor=1):
        proxy_data = None
        if self.proxy_obj:
            proxy_data = {
                "https": f"http://{self.config.proxy_string}"
            }

        for attempt in range(1, retries + 1):
            if self.stop_event and self.stop_event.is_set():
                return

            try:
                response = self.session.get(
                    url=url,
                    headers=self.headers,
                    proxies=proxy_data,
                    cookies=self.cookies,
                    impersonate="chrome",
                    timeout=20,
                    verify=False,
                )
                logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt}: {response.status_code}")

                if response.status_code >= 500:
                    raise requests.RequestsError(f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {response.status_code}")
                if response.status_code in [302, 403, 429]:
                    self.bad_request_count += 1
                    logger.warning(f"‚ö†Ô∏è –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ Avito: –∫–æ–¥ {response.status_code}")
                    self.session = requests.Session()
                    if attempt >= 3:
                        self.cookies = self.get_cookies()
                    self.change_ip()
                    if response.status_code == 429:
                        backoff_time = min(attempt * 10, 60)  # 10, 20, 30 —Å–µ–∫ (–º–∞–∫—Å 60)
                        logger.warning(f"‚ö†Ô∏è –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ 429! –ü–∞—É–∑–∞ {backoff_time} —Å–µ–∫...")
                        time.sleep(backoff_time)
                    raise requests.RequestsError(f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {response.status_code}")

                self.save_cookies()
                self.good_request_count += 1
                return response.text
            except requests.RequestsError as e:
                logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt} –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å –Ω–µ—É—Å–ø–µ—à–Ω–æ: {e}")
                if attempt < retries:
                    sleep_time = backoff_factor * attempt
                    logger.debug(f"–ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {sleep_time} —Å–µ–∫—É–Ω–¥...")
                    time.sleep(sleep_time)
                else:
                    logger.info("–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –±—ã–ª–∏ –Ω–µ—É—Å–ø–µ—à–Ω—ã–º–∏")
                    return None

    def parse(self):
        if self.config.one_file_for_link:
            self.xlsx_handler = None

        for _index, url in enumerate(self.config.urls):
            ads_in_link = []
            for i in range(0, self.config.count):
                if self.stop_event and self.stop_event.is_set():
                    return
                if DEBUG_MODE:
                    html_code = open("december.txt", "r", encoding="utf-8").read()
                else:
                    html_code = self.fetch_data(url=url, retries=self.config.max_count_of_retry)

                if not html_code:
                    logger.warning(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML –¥–ª—è {url}, –ø—Ä–æ–±—É—é –∑–∞–Ω–æ–≤–æ —á–µ—Ä–µ–∑ {self.config.pause_between_links} —Å–µ–∫.")
                    time.sleep(self.config.pause_between_links)
                    continue

                if not self.xlsx_handler and self.config.one_file_for_link:
                    self.xlsx_handler = XLSXHandler(f"result/{_index + 1}.xlsx")

                data_from_page = self.find_json_on_page(html_code=html_code)
                try:
                    catalog = data_from_page.get("data", {}).get("catalog") or {}
                    ads_models = ItemsResponse(**catalog)
                except ValidationError as err:
                    logger.error(f"–ü—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {err}")
                    continue

                ads = self._clean_null_ads(ads=ads_models.items)

                ads = self._add_seller_to_ads(ads=ads)

                if not ads:
                    logger.info("–û–±—ä—è–≤–ª–µ–Ω–∏—è –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å, –∑–∞–∫–∞–Ω—á–∏–≤–∞—é —Ä–∞–±–æ—Ç—É —Å –¥–∞–Ω–Ω–æ–π —Å—Å—ã–ª–∫–æ–π")
                    break

                filter_ads = self.filter_ads(ads=ads)

                if self.tg_handler and not self.config.one_time_start:
                    self._send_to_tg(ads=filter_ads)

                if self.vk_handler and not self.config.one_time_start:
                    self._send_to_vk(ads=filter_ads)

                filter_ads = self.parse_views(ads=filter_ads)

                if filter_ads:
                    self.__save_viewed(ads=filter_ads)

                    if self.config.save_xlsx:
                        ads_in_link.extend(filter_ads)

                url = self.get_next_page_url(url=url)

                logger.info(f"–ü–∞—É–∑–∞ {self.config.pause_between_links} —Å–µ–∫.")
                time.sleep(self.config.pause_between_links)

            if ads_in_link:
                logger.info(f"–°–æ—Ö—Ä–∞–Ω—è—é –≤ Excel {len(ads_in_link)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
                self.__save_data(ads=ads_in_link)
            else:
                logger.info("–°–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–µ—á–µ–≥–æ")

            if self.config.one_file_for_link:
                self.xlsx_handler = None

        logger.info(f"–•–æ—Ä–æ—à–∏–µ –∑–∞–ø—Ä–æ—Å—ã: {self.good_request_count}—à—Ç, –ø–ª–æ—Ö–∏–µ: {self.bad_request_count}—à—Ç")

        if self.config.one_time_start and self.tg_handler:
            self.tg_handler.send_to_tg(msg="–ü–∞—Ä—Å–∏–Ω–≥ –ê–≤–∏—Ç–æ –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Å–µ —Å—Å—ã–ª–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
            self.stop_event = True

        if self.config.one_time_start and self.vk_handler:
            self.vk_handler.send_to_vk(msg="–ü–∞—Ä—Å–∏–Ω–≥ –ê–≤–∏—Ç–æ –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Å–µ —Å—Å—ã–ª–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
            self.stop_event = True

    @staticmethod
    def _clean_null_ads(ads: list[Item]) -> list[Item]:
        return [ad for ad in ads if ad.id]

    @staticmethod
    def find_json_on_page(html_code, data_type: str = "mime") -> dict:
        soup = BeautifulSoup(html_code, "html.parser")
        try:
            for _script in soup.select('script'):
                script_type = _script.get('type')

                if data_type == 'mime' and script_type == 'mime/invalid':
                    script_content = html.unescape(_script.text)
                    parsed_data = json.loads(script_content)

                    if 'state' in parsed_data:
                        return parsed_data['state']

                    elif 'data' in parsed_data:
                        logger.info("data")
                        return parsed_data['data']

                    else:
                        return parsed_data

        except Exception as err:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {err}")
        return {}

    def filter_ads(self, ads: list[Item]) -> list[Item]:
        """–°–æ—Ä—Ç–∏—Ä—É–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        ads = self._parse_area_from_description(ads)
        filters = [
            self._filter_viewed,
            self._filter_by_price_range,
            self._filter_by_black_keywords,
            self._filter_by_white_keyword,
            self._filter_by_address,
            self._filter_by_seller,
            self._filter_by_recent_time,
            self._filter_by_reserve,
            self._filter_by_promotion,
        ]

        for filter_fn in filters:
            ads = filter_fn(ads)
            logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ {filter_fn.__name__} –æ—Å—Ç–∞–ª–æ—Å—å {len(ads)}")
            if not len(ads):
                return ads
        return ads

    def _filter_by_price_range(self, ads: list[Item]) -> list[Item]:
        try:
            return [ad for ad in ads if self.config.min_price <= ad.priceDetailed.value <= self.config.max_price]
        except Exception as err:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Ü–µ–Ω–µ: {err}")
            return ads

    def _filter_by_black_keywords(self, ads: list[Item]) -> list[Item]:
        if not self.config.keys_word_black_list:
            return ads
        try:
            return [ad for ad in ads if not self._is_phrase_in_ads(ad=ad, phrases=self.config.keys_word_black_list)]
        except Exception as err:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ —Å–ø–∏—Å–∫—É —Å—Ç–æ–ø-—Å–ª–æ–≤: {err}")
            return ads

    def _filter_by_white_keyword(self, ads: list[Item]) -> list[Item]:
        if not self.config.keys_word_white_list:
            return ads
        try:
            return [ad for ad in ads if self._is_phrase_in_ads(ad=ad, phrases=self.config.keys_word_white_list)]
        except Exception as err:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ —Å–ø–∏—Å–∫—É –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {err}")
            return ads

    def _filter_by_address(self, ads: list[Item]) -> list[Item]:
        if not self.config.geo:
            return ads
        try:
            return [ad for ad in ads if self.config.geo in ad.geo.formattedAddress]
        except Exception as err:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ –∞–¥—Ä–µ—Å—É: {err}")
            return ads

    def _filter_viewed(self, ads: list[Item]) -> list[Item]:
        try:
            return [ad for ad in ads if not self.is_viewed(ad=ad)]
        except Exception as err:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø–æ –ø—Ä–∏–∑–Ω–∞–∫—É —Å–º–æ—Ç—Ä–µ–ª–∏ –∏–ª–∏ –Ω–µ —Å–º–æ—Ç—Ä–µ–ª–∏: {err}")
            return ads

    def _add_seller_to_ads(self, ads: list[Item]) -> list[Item]:
        for ad in ads:
            if seller_id := self._extract_seller_slug(data=ad):
                ad.sellerId = seller_id
        return ads

    @staticmethod
    def _add_promotion_to_ads(ads: list[Item]) -> list[Item]:
        for ad in ads:
            ad.isPromotion = any(
                v.get("title") == "–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ"
                for step in (ad.iva or {}).get("DateInfoStep", [])
                for v in step.payload.get("vas", [])
            )
        return ads

    def _filter_by_seller(self, ads: list[Item]) -> list[Item]:
        if not self.config.seller_black_list:
            return ads
        try:
            return [ad for ad in ads if not ad.sellerId or ad.sellerId not in self.config.seller_black_list]
        except Exception as err:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç—Å–µ–∏–≤–∞–Ω–∏–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –ø—Ä–æ–¥–∞–≤—Ü–∞–º–∏ –∏–∑ —á–µ—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ : {err}")
            return ads

    def _filter_by_recent_time(self, ads: list[Item]) -> list[Item]:
        if not self.config.max_age:
            return ads
        try:
            return [ad for ad in ads if
                    self._is_recent(timestamp_ms=ad.sortTimeStamp, max_age_seconds=self.config.max_age)]
        except Exception as err:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç—Å–µ–∏–≤–∞–Ω–∏–∏ —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {err}")
            return ads

    def _filter_by_reserve(self, ads: list[Item]) -> list[Item]:
        if not self.config.ignore_reserv:
            return ads
        try:
            return [ad for ad in ads if not ad.isReserved]
        except Exception as err:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç—Å–µ–∏–≤–∞–Ω–∏–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ —Ä–µ–∑–µ—Ä–≤–µ: {err}")
            return ads

    def _filter_by_promotion(self, ads: list[Item]) -> list[Item]:
        ads = self._add_promotion_to_ads(ads=ads)
        if not self.config.ignore_promotion:
            return ads
        try:
            return [ad for ad in ads if not ad.isPromotion]
        except Exception as err:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç—Å–µ–∏–≤–∞–Ω–∏–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {err}")
            return ads

    def parse_views(self, ads: list[Item]) -> list[Item]:
        if not self.config.parse_views:
            return ads

        logger.info(f"üîç –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –¥–ª—è {len(ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")

        for index, ad in enumerate(ads, 1):
            try:
                logger.info(f"üìä [{index}/{len(ads)}] ID: {ad.id}, URL: {ad.urlPath}")  # ‚Üê –î–û–ë–ê–í–¨

                html_code_full_page = self.fetch_data(url=f"https://www.avito.ru{ad.urlPath}")

                # –î–û–ë–ê–í–¨ –ü–†–û–í–ï–†–ö–£
                if not html_code_full_page:
                    logger.warning(f"‚ö†Ô∏è HTML –ø—É—Å—Ç–æ–π –¥–ª—è {ad.urlPath}, –ø—Ä–æ–ø—É—Å–∫–∞—é")
                    continue

                logger.debug(f"‚úÖ HTML –ø–æ–ª—É—á–µ–Ω: {len(html_code_full_page)} –±–∞–π—Ç")  # ‚Üê –î–û–ë–ê–í–¨

                ad.total_views, ad.today_views = self._extract_views(html=html_code_full_page)

                logger.debug(f"üìà –ü—Ä–æ—Å–º–æ—Ç—Ä—ã: –≤—Å–µ–≥–æ={ad.total_views}, —Å–µ–≥–æ–¥–Ω—è={ad.today_views}")  # ‚Üê –î–û–ë–ê–í–¨

                delay = random.uniform(0.9, 2.5)
                logger.debug(f"‚è∏Ô∏è –ü–∞—É–∑–∞ {delay:.1f} —Å–µ–∫...")
                time.sleep(delay)

            except Exception as err:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {ad.urlPath}: {err}", exc_info=True)  # ‚Üê –î–û–ë–ê–í–¨ exc_info
                continue

        logger.info("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω")
        return ads

    @staticmethod
    def _extract_views(html: str) -> tuple:
        soup = BeautifulSoup(html, "html.parser")

        def extract_digits(element):
            return int(''.join(filter(str.isdigit, element.get_text()))) if element else None

        total = extract_digits(soup.select_one('[data-marker="item-view/total-views"]'))
        today = extract_digits(soup.select_one('[data-marker="item-view/today-views"]'))

        return total, today

    def change_ip(self) -> bool:
        if not self.config.proxy_change_url:
            logger.info("–°–µ–π—á–∞—Å –±—ã –±—ã–ª–∞ —Å–º–µ–Ω–∞ ip, –Ω–æ –º—ã –±–µ–∑ –ø—Ä–æ–∫—Å–∏")
            return False
        logger.info("–ú–µ–Ω—è—é IP")
        try:
            res = requests.get(url=self.config.proxy_change_url, verify=False)
            if res.status_code == 200:
                logger.info("IP –∏–∑–º–µ–Ω–µ–Ω")
                return True
        except Exception as err:
            logger.info(f"–ü—Ä–∏ —Å–º–µ–Ω–µ ip –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞: {err}")
        logger.info("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–º–µ–Ω–∏—Ç—å IP, –ø—Ä–æ–±—É—é –µ—â–µ —Ä–∞–∑")
        time.sleep(random.randint(3, 10))
        return self.change_ip()

    @staticmethod
    def _extract_seller_slug(data):
        match = re.search(r"/brands/([^/?#]+)", str(data))
        if match:
            return match.group(1)
        return None

    @staticmethod
    def _is_phrase_in_ads(ad: Item, phrases: list) -> bool:
        full_text_from_ad = (ad.title + ad.description).lower()
        return any(phrase.lower() in full_text_from_ad for phrase in phrases)

    def is_viewed(self, ad: Item) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–º–æ—Ç—Ä–µ–ª–∏ –º—ã —ç—Ç–æ –∏–ª–∏ –Ω–µ—Ç"""
        return self.db_handler.record_exists(record_id=ad.id, price=ad.priceDetailed.value)

    @staticmethod
    def _is_recent(timestamp_ms: int, max_age_seconds: int) -> bool:
        now = datetime.utcnow()
        published_time = datetime.utcfromtimestamp(timestamp_ms / 1000)
        return (now - published_time) <= timedelta(seconds=max_age_seconds)

    def __get_file_title(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        title_file = 'all'
        if self.config.keys_word_white_list:
            title_file = "-".join(list(map(str.lower, self.config.keys_word_white_list)))
            if len(title_file) > 50:
                title_file = title_file[:50]

        return f"result/{title_file}.xlsx"

    def __save_data(self, ads: list[Item]) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–∞–π–ª keyword*.xlsx –∏ –≤ –ë–î"""
        try:
            self.xlsx_handler.append_data_from_page(ads=ads)
        except Exception as err:
            logger.info(f"–ü—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ Excel –æ—à–∏–±–∫–∞ {err}")

    def __save_viewed(self, ads: list[Item]) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        try:
            self.db_handler.add_record_from_page(ads=ads)
        except Exception as err:
            logger.info(f"–ü—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –ë–î –æ—à–∏–±–∫–∞ {err}")

    def get_next_page_url(self, url: str):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            url_parts = urlparse(url)
            query_params = parse_qs(url_parts.query)
            current_page = int(query_params.get('p', [1])[0])
            query_params['p'] = current_page + 1
            if self.config.one_time_start:
                logger.debug(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page}")

            new_query = urlencode(query_params, doseq=True)
            next_url = urlunparse((url_parts.scheme, url_parts.netloc, url_parts.path, url_parts.params, new_query,
                                   url_parts.fragment))
            return next_url
        except Exception as err:
            logger.error(f"–ù–µ —Å–º–æ–≥ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è {url}. –û—à–∏–±–∫–∞: {err}")


def extract_area_from_description(description: str) -> float | None:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–ª–æ—â–∞–¥—å –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è Avito

    –ü—Ä–∏–º–µ—Ä—ã:
    - "120 –º¬≤" ‚Üí 120.0
    - "85,5 –∫–≤.–º" ‚Üí 85.5
    - "–ü–ª–æ—â–∞–¥—å 200 –º2" ‚Üí 200.0
    """
    if not description:
        return None

    import re

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–ª–æ—â–∞–¥–∏
    patterns = [
        r'(\d+(?:[.,]\d+)?)\s*–º[¬≤2]',  # "120 –º¬≤" –∏–ª–∏ "120 –º2"
        r'(\d+(?:[.,]\d+)?)\s*–∫–≤\.?\s*–º',  # "120 –∫–≤.–º" –∏–ª–∏ "120 –∫–≤ –º"
        r'–ø–ª–æ—â–∞–¥—å[:\s]+(\d+(?:[.,]\d+)?)',  # "–ü–ª–æ—â–∞–¥—å: 120" –∏–ª–∏ "–ø–ª–æ—â–∞–¥—å 120"
        r'(\d+(?:[.,]\d+)?)\s*–∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö –º–µ—Ç—Ä–æ–≤',  # "120 –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö –º–µ—Ç—Ä–æ–≤"
    ]

    for pattern in patterns:
        match = re.search(pattern, description, re.IGNORECASE)
        if match:
            area_str = match.group(1).replace(',', '.')
            try:
                area = float(area_str)
                # –§–∏–ª—å—Ç—Ä: –ø–ª–æ—â–∞–¥—å –æ–±—ã—á–Ω–æ –æ—Ç 10 –¥–æ 10000 –º¬≤
                if 10 <= area <= 10000:
                    logger.debug(f"üí° –ü–ª–æ—â–∞–¥—å –Ω–∞–π–¥–µ–Ω–∞: {area} –º¬≤")
                    return area
            except ValueError:
                continue

    return None

if __name__ == "__main__":
    try:
        config = load_avito_config("config.toml")
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞: {err}")
        exit(1)

    while True:
        try:
            parser = AvitoParse(config)
            parser.parse()
            if config.one_time_start:
                logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —Ç.–∫. –≤–∫–ª—é—á—ë–Ω one_time_start –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö")
                break
            logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ü–∞—É–∑–∞ {config.pause_general} —Å–µ–∫")
            time.sleep(config.pause_general)
        except Exception as err:
            logger.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ {err}. –ë—É–¥–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ 30 —Å–µ–∫.")
            time.sleep(30)
